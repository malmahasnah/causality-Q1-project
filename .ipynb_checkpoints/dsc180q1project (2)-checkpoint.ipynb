{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef8d684a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import us  # Import the us library\n",
    "\n",
    "from causallearn.search.ConstraintBased.PC import pc\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "181bc791",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/online_shoppers_intention.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10816\\154192336.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mshopper_intent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/online_shoppers_intention.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mshopper_intent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/online_shoppers_intention.csv'"
     ]
    }
   ],
   "source": [
    "shopper_intent = pd.read_csv(\"data/online_shoppers_intention.csv\")\n",
    "shopper_intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3309fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "shopper_intent.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3385860a",
   "metadata": {},
   "outputs": [],
   "source": [
    "shopper_intent['SpecialDay'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e384e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "shopper_intent['Revenue'] = shopper_intent['Revenue'].astype(int)\n",
    "shopper_intent['Weekend'] = shopper_intent['Weekend'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f37d1f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_graph = shopper_intent.drop(['Month'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970450a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_use = [\n",
    "    'SpecialDay', 'Revenue', 'BounceRates', 'ExitRates',\n",
    "    'Weekend', 'Administrative', 'Administrative_Duration',\n",
    "    'Informational', 'Informational_Duration', 'ProductRelated',\n",
    "    'ProductRelated_Duration', 'OperatingSystems', 'Browser', 'Region',\n",
    "    'TrafficType','PageValues',\n",
    "    'VisitorType_Other', 'VisitorType_Returning_Visitor'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb381648",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded = pd.get_dummies(data_graph, drop_first=True)\n",
    "data_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cb8438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a long time to run\n",
    "data_frame = data_encoded\n",
    "# Step 1: Convert DataFrame to NumPy array>\n",
    "data_array = data_frame.to_numpy()\n",
    "\n",
    "# Step 2: Apply the PC algorithm to discover the causal graph\n",
    "alpha = 0.05  # Significance level\n",
    "pc_graph = pc(data_array, alpha)\n",
    "\n",
    "# Step 3: Create labels for nodes based on DataFrame columns\n",
    "node_labels = {i: col for i, col in enumerate(data_encoded.columns)}\n",
    "\n",
    "# Step 4: Extract edges from the pc_graph and create a NetworkX directed graph\n",
    "G = nx.DiGraph()\n",
    "G.add_nodes_from(node_labels.keys())\n",
    "\n",
    "# Add edges based on the adjacency matrix\n",
    "for i in range(len(pc_graph.G.graph)):\n",
    "    for j in range(len(pc_graph.G.graph)):\n",
    "        if pc_graph.G.graph[i, j] != 0:  # Check for an edge\n",
    "            if pc_graph.G.graph[j, i] == 1 and pc_graph.G.graph[i, j] == -1:\n",
    "                # Case: i -> j\n",
    "                G.add_edge(i, j, edge_type='directed')\n",
    "            elif pc_graph.G.graph[j, i] == -1 and pc_graph.G.graph[i, j] == -1:\n",
    "                # Case: i -- j (undirected)\n",
    "                G.add_edge(i, j, edge_type='undirected')\n",
    "            elif pc_graph.G.graph[j, i] == 1 and pc_graph.G.graph[i, j] == 1:\n",
    "                # Case: i <-> j (bidirectional)\n",
    "                G.add_edge(i, j, edge_type='bidirectional')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f11f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Plot the graph\n",
    "plt.figure(figsize=(10, 8))\n",
    "pos = nx.spring_layout(G, k=0.5, iterations=50)  # Increase k for more space between nodes\n",
    "nx.draw(G, pos, with_labels=True, labels=node_labels, node_size=1000, node_color=\"skyblue\", font_size=7, font_weight=\"bold\", arrowstyle=\"->\", arrowsize=20)\n",
    "plt.title(\"Inferred Causal Graph\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff1382f",
   "metadata": {},
   "source": [
    "## Let's find the nodes only related to Revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9652b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the index of the \"Revenue\" node\n",
    "revenue_index = list(data_encoded.columns).index(\"Revenue\")\n",
    "\n",
    "# Get the adjacency matrix from the PC graph\n",
    "graph_matrix = pc_graph.G.graph  # Assuming this is a NumPy array or similar\n",
    "\n",
    "# Initialize sets for storing relationships\n",
    "parents = set()\n",
    "children = set()\n",
    "spouses = set()\n",
    "edges = []  # Store edges for graph visualization\n",
    "\n",
    "# First Loop: Find direct relationships (parents and children)\n",
    "for j in range(graph_matrix.shape[0]):\n",
    "    if j == revenue_index:\n",
    "        continue  # Skip self-loops\n",
    "\n",
    "    if graph_matrix[j, revenue_index] == 1 and graph_matrix[revenue_index, j] == -1:\n",
    "        # Directed edge (Revenue → j): j is a child\n",
    "        children.add(j)\n",
    "        edges.append((revenue_index, j))\n",
    "    elif graph_matrix[revenue_index, j] == 1 and graph_matrix[j, revenue_index] == -1:\n",
    "        # Directed edge (j → Revenue): j is a parent\n",
    "        parents.add(j)\n",
    "        edges.append((j, revenue_index))\n",
    "\n",
    "# Second Loop: Identify spouses by checking for shared children\n",
    "for child in children:\n",
    "    for j in range(graph_matrix.shape[0]):\n",
    "        if j == revenue_index or j in children:\n",
    "            continue  # Skip self-loops and already identified children\n",
    "\n",
    "        if graph_matrix[j, child] == 1 and graph_matrix[child, j] == -1:\n",
    "            # Directed edge (child → j): j shares a child with Revenue\n",
    "            spouses.add(j)\n",
    "            edges.append((j, child))  # Add directed edge from spouse to the child\n",
    "\n",
    "# Create a subgraph with only the related nodes (parents, children, spouses)\n",
    "related_nodes = {revenue_index} | parents | children | spouses\n",
    "\n",
    "# Create a directed graph\n",
    "subgraph = nx.DiGraph()\n",
    "subgraph.add_nodes_from(related_nodes)\n",
    "subgraph.add_edges_from(edges)\n",
    "\n",
    "# Map node indices to column names for readability\n",
    "node_labels = list(data_encoded.columns)\n",
    "subgraph = nx.relabel_nodes(subgraph, {i: node_labels[i] for i in related_nodes})\n",
    "\n",
    "# Draw the subgraph\n",
    "pos = nx.spring_layout(subgraph)\n",
    "nx.draw(subgraph, pos, with_labels=True, node_size=1000, font_size=10, node_color=\"skyblue\", arrowsize=20)\n",
    "plt.title(\"Causal Relationships for Revenue\")\n",
    "plt.show()\n",
    "\n",
    "# Print the results\n",
    "print(\"Parents of Revenue:\", [node_labels[i] for i in parents])\n",
    "print(\"Children of Revenue:\", [node_labels[i] for i in children])\n",
    "print(\"Spouses of Revenue:\", [node_labels[i] for i in spouses])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9641c77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define children, spouses, and discrete variable\n",
    "children = [\n",
    "    \"Administrative\", \"ProductRelated_Duration\", \"BounceRates\",\n",
    "    \"ExitRates\", \"PageValues\", \"SpecialDay\", \"VisitorType_Returning_Visitor\"\n",
    "]\n",
    "spouses = [\"Administrative_Duration\", \"Informational\", \"OperatingSystems\"]\n",
    "\n",
    "# Continuous variables\n",
    "continuous_vars = [var for var in children + spouses if var != \"VisitorType_Returning_Visitor\"]\n",
    "\n",
    "# Create a figure with subplots\n",
    "num_vars = len(children) + len(spouses)\n",
    "cols = 3  # Number of columns in the grid\n",
    "rows = -(-num_vars // cols)  # Ceiling division\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(15, rows * 5), constrained_layout=True)\n",
    "\n",
    "# Flatten axes for easier indexing\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot continuous variables\n",
    "for i, var in enumerate(continuous_vars):\n",
    "    sns.boxplot(x=\"Revenue\", y=var, data=data_encoded, ax=axes[i], palette=\"Set2\")\n",
    "    axes[i].set_title(f\"{var} vs Revenue\")\n",
    "    axes[i].set_xlabel(\"Revenue\")\n",
    "    axes[i].set_ylabel(var)\n",
    "\n",
    "# Plot discrete variable\n",
    "visitor_var = \"VisitorType_Returning_Visitor\"\n",
    "\n",
    "# Group data to calculate counts\n",
    "visitor_revenue_counts = data_encoded.groupby([\"Revenue\", visitor_var]).size().unstack()\n",
    "\n",
    "# Plot stacked bar chart\n",
    "visitor_revenue_counts.plot(\n",
    "    kind=\"bar\", \n",
    "    stacked=True, \n",
    "    colormap=\"viridis\", \n",
    "    alpha=0.8,\n",
    "    ax=axes[len(continuous_vars)]\n",
    ")\n",
    "\n",
    "axes[len(continuous_vars)].set_title(f\"{visitor_var} vs Revenue\")\n",
    "axes[len(continuous_vars)].set_xlabel(\"Revenue\")\n",
    "axes[len(continuous_vars)].set_ylabel(f\"Count\")\n",
    "\n",
    "# Hide any remaining empty subplots\n",
    "for j in range(len(continuous_vars) + 1, len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "\n",
    "# Show the figure\n",
    "plt.suptitle(\"Relationships Between Variables and Revenue\", fontsize=16, y=1.02)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4654f9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns for parents, children, and spouses\n",
    "selected_features = [\n",
    "    \"Administrative\", \"ProductRelated_Duration\", \"BounceRates\", \"ExitRates\",\n",
    "    \"PageValues\", \"SpecialDay\", \"VisitorType_Returning_Visitor\",\n",
    "    \"Administrative_Duration\", \"Informational\", \"OperatingSystems\"\n",
    "]\n",
    "\n",
    "# Prepare the data\n",
    "X = data_encoded.drop(columns=[\"Revenue\"])  # All features except target\n",
    "y = data_encoded[\"Revenue\"]                # Target variable\n",
    "\n",
    "# Scale continuous features for logistic regression\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Model 1: Logistic Regression with all variables\n",
    "model_all = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model_all.fit(X_train, y_train)\n",
    "\n",
    "# Model 2: Logistic Regression with selected variables\n",
    "X_train_selected = X_train[selected_features]\n",
    "X_test_selected = X_test[selected_features]\n",
    "model_selected = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_all = model_all.predict(X_test)\n",
    "y_pred_selected = model_selected.predict(X_test_selected)\n",
    "\n",
    "# Metrics\n",
    "def evaluate_model(y_true, y_pred, y_prob=None):\n",
    "    metrics = {\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"Precision\": precision_score(y_true, y_pred),\n",
    "        \"Recall\": recall_score(y_true, y_pred),\n",
    "        \"F1 Score\": f1_score(y_true, y_pred),\n",
    "    }\n",
    "    if y_prob is not None:\n",
    "        metrics[\"AUC-ROC\"] = roc_auc_score(y_true, y_prob)\n",
    "    return metrics\n",
    "\n",
    "# Evaluate both models\n",
    "y_prob_all = model_all.predict_proba(X_test)[:, 1]\n",
    "y_prob_selected = model_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "metrics_all = evaluate_model(y_test, y_pred_all, y_prob_all)\n",
    "metrics_selected = evaluate_model(y_test, y_pred_selected, y_prob_selected)\n",
    "\n",
    "# Display results\n",
    "print(\"Model with All Variables\")\n",
    "for metric, value in metrics_all.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nModel with Selected Variables\")\n",
    "for metric, value in metrics_selected.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Classification reports\n",
    "print(\"\\nClassification Report (All Variables):\\n\", classification_report(y_test, y_pred_all))\n",
    "print(\"\\nClassification Report (Selected Variables):\\n\", classification_report(y_test, y_pred_selected))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efe95a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc8147e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6426990d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
